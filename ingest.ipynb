{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/localGPT_demo/SOURCE_DOCUMENTS intfloat/e5-small-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "cpu\n",
      "client=INSTRUCTOR(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n",
      "  (2): Normalize()\n",
      ") model_name='intfloat/e5-small-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={} embed_instruction='Represent the document for retrieval: ' query_instruction='Represent the question for retrieving supporting documents: '\n",
      "neo4j\n",
      "rRZAanB5uC7y-0CePWU_YJhReIEdg8sNsOqOa5MOiW4\n",
      "neo4j+s://96690cc7.databases.neo4j.io\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "\n",
    "import click\n",
    "import torch\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "from constants import (\n",
    "    DOCUMENT_MAP,\n",
    "    EMBEDDING_MODEL_NAME,\n",
    "    INGEST_THREADS,\n",
    "    PERSIST_DIRECTORY,\n",
    "    SOURCE_DIRECTORY,\n",
    ")\n",
    "\n",
    "\n",
    "def load_single_document(file_path: str) -> Document:\n",
    "    # Loads a single document from a file path\n",
    "    file_extension = os.path.splitext(file_path)[1]\n",
    "    loader_class = DOCUMENT_MAP.get(file_extension)\n",
    "    if loader_class:\n",
    "        loader = loader_class(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Document type is undefined\")\n",
    "    return loader.load()[0]\n",
    "\n",
    "\n",
    "def load_document_batch(filepaths):\n",
    "    logging.info(\"Loading document batch\")\n",
    "    # create a thread pool\n",
    "    with ThreadPoolExecutor(len(filepaths)) as exe:\n",
    "        # load files\n",
    "        futures = [exe.submit(load_single_document, name) for name in filepaths]\n",
    "        # collect data\n",
    "        data_list = [future.result() for future in futures]\n",
    "        # return data and file paths\n",
    "        return (data_list, filepaths)\n",
    "\n",
    "\n",
    "def load_documents(source_dir: str) -> list[Document]:\n",
    "    # Loads all documents from the source documents directory, including nested folders\n",
    "    paths = []\n",
    "    for root, _, files in os.walk(source_dir):\n",
    "        for file_name in files:\n",
    "            file_extension = os.path.splitext(file_name)[1]\n",
    "            source_file_path = os.path.join(root, file_name)\n",
    "            if file_extension in DOCUMENT_MAP.keys():\n",
    "                paths.append(source_file_path)\n",
    "\n",
    "    # Have at least one worker and at most INGEST_THREADS workers\n",
    "    n_workers = min(INGEST_THREADS, max(len(paths), 1))\n",
    "    chunksize = round(len(paths) / n_workers)\n",
    "    docs = []\n",
    "    with ProcessPoolExecutor(n_workers) as executor:\n",
    "        futures = []\n",
    "        # split the load operations into chunks\n",
    "        for i in range(0, len(paths), chunksize):\n",
    "            # select a chunk of filenames\n",
    "            filepaths = paths[i : (i + chunksize)]\n",
    "            # submit the task\n",
    "            future = executor.submit(load_document_batch, filepaths)\n",
    "            futures.append(future)\n",
    "        # process all results\n",
    "        for future in as_completed(futures):\n",
    "            # open the file and load the data\n",
    "            contents, _ = future.result()\n",
    "            docs.extend(contents)\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def split_documents(documents: list[Document]) -> tuple[list[Document], list[Document]]:\n",
    "    # Splits documents for correct Text Splitter\n",
    "    text_docs, python_docs = [], []\n",
    "    for doc in documents:\n",
    "        file_extension = os.path.splitext(doc.metadata[\"source\"])[1]\n",
    "        if file_extension == \".py\":\n",
    "            python_docs.append(doc)\n",
    "        else:\n",
    "            text_docs.append(doc)\n",
    "\n",
    "    return text_docs, python_docs\n",
    "\n",
    "def main(device_type):\n",
    "    # Load documents and split in chunks\n",
    "    logging.info(f\"Loading documents from {SOURCE_DIRECTORY}\")\n",
    "    print(SOURCE_DIRECTORY,EMBEDDING_MODEL_NAME)\n",
    "    documents = load_documents(SOURCE_DIRECTORY)\n",
    "    text_documents, python_documents = split_documents(documents)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.PYTHON, chunk_size=880, chunk_overlap=200\n",
    "    )\n",
    "    texts = text_splitter.split_documents(text_documents)\n",
    "    texts.extend(python_splitter.split_documents(python_documents))\n",
    "    logging.info(f\"Loaded {len(documents)} documents from {SOURCE_DIRECTORY}\")\n",
    "    logging.info(f\"Split into {len(texts)} chunks of text\")\n",
    "    url = \"neo4j+s://96690cc7.databases.neo4j.io\"\n",
    "    username = \"neo4j\"\n",
    "    password = \"rRZAanB5uC7y-0CePWU_YJhReIEdg8sNsOqOa5MOiW4\"\n",
    "    # Wait 60 seconds before connecting using these details, or login to https://console.neo4j.io to validate the Aura Instance is available\n",
    "    # NEO4J_URI=neo4j+s://96690cc7.databases.neo4j.io\n",
    "    # NEO4J_USERNAME=neo4j\n",
    "    # NEO4J_PASSWORD=rRZAanB5uC7y-0CePWU_YJhReIEdg8sNsOqOa5MOiW4\n",
    "    # AURA_INSTANCEID=96690cc7\n",
    "    # AURA_INSTANCENAME=Instance01\n",
    "\n",
    "    # Create embeddings\n",
    "    embeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": device_type})\n",
    "    print(device_type)\n",
    "    print(embeddings)\n",
    "    print(username)\n",
    "    print(password)\n",
    "    print(url)\n",
    "    # print(texts)\n",
    "    db = Neo4jVector.from_documents(\n",
    "    texts, embeddings, url=url, username=username, password=password\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "    # change the embedding type here if you are running into issues.\n",
    "    # These are much smaller embeddings and will work for most appications\n",
    "    # If you use HuggingFaceEmbeddings, make sure to also use the same in the\n",
    "    # run_localGPT.py file.\n",
    "\n",
    "    # embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "device_type = 'cpu'\n",
    "main(device_type)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
